{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment Points**: 100  \n",
    "**Assignment Weight**: 10%  \n",
    "**Submission**: Submit your file using the link below\n",
    "\n",
    "\n",
    "Put **all your work** into a file titled `BUSA8001_programming_task2_MQ_ID.ipynb` where MQ_ID is your Macquarie University student ID number (e.g. if MQ_ID == 12345678 then you need to submit BUSA8001_programming_task2_12345678.ipynb). \n",
    "\n",
    "Failure to submit a correctly named file will result in a loss of 3 marks from the assignment total.\n",
    "\n",
    "- **Do NOT print your variables to screen unless explicitly asked to do so** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1 - (30 points) {-}\n",
    "\n",
    "1. Import the credit card data from https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls into a pandas DataFrame named `df` making sure you skip the top row when reading the data. Delete the 'ID\" column after importing the data. (5 points)\n",
    "\n",
    "2. Rename the column 'PAY_0' to 'PAY_1' and the column 'default payment next month' to 'payment_default' (5 points)\n",
    "\n",
    "3. Create a one-dimensional NumPy array named `y` by exporting the first 250 observations of 'payment_default' column from df (hint: if stuck see `ravel` NumPy method). Similarly, create a two-dimensional NumPy array named `X` by exporting the first 250 observatations of 'PAY_1', 'AGE', 'SEX', 'MARRIAGE', 'EDUCATION' and 'BILL_AMT1' columns. (10 points)\n",
    "\n",
    "4. Use an appropriate `scikit-learn` library we learned in class to create the following NumPy arrays: `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 70% train and 30% test datasets. Set `random_state` to 1 and stratify subsamples so that train and test datasets have roughly equal proportions of the target's class labels. (5 points) \n",
    "\n",
    "5. Use an appropriate `scikit-learn` library we learned in class to standardize features from train and test datasets to mean zero and variance 1. Make sure you train the scaler only on train data. (5 points)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your code for Problem 1 in the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, \"display.width\", None) # pretty printing\n",
    "\n",
    "#1. import credit card data to df and skip the top row \n",
    "df = pd.read_excel('https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls', skiprows=[0])\n",
    "df.shape\n",
    "#print(df.head(10))\n",
    "\n",
    "#Delete the 'ID\" column after importing the data\n",
    "df.drop('ID', inplace=True, axis=1)\n",
    "#print(df.head(10))\n",
    "\n",
    "#2. Rename columns\n",
    "df = df.rename(columns = {'PAY_0':'PAY_1', 'default payment next month': 'payment_default'})\n",
    "#print(df.head(10))\n",
    "\n",
    "#3. Create one-dimensional numpy array y and 2-dimensional numpy array X\n",
    "y = df['payment_default'].loc[:249].values #.values is to transfer the value from payment_default to array y\n",
    "#print(y, len(y))\n",
    "\n",
    "colums_to_use_for_X = ['PAY_1', 'AGE', 'SEX', 'MARRIAGE', 'EDUCATION', 'BILL_AMT1']\n",
    "X = df[colums_to_use_for_X].loc[:249].values\n",
    "#print(X,len(X))\n",
    "\n",
    "#4. Splitting dtat into 70% train and 30% test datasets; random_state = 1 and stratify = y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1, stratify = y)\n",
    "\n",
    "# print('X train shape:', X_train.shape)\n",
    "# print('y train shape:', y.shape)\n",
    "# print('X test shape:', X_test.shape)\n",
    "# print('y test shape:', y_test.shape)\n",
    "# print(y_train)\n",
    "# #check to see train and test datasets have roughly equal proportion of target's class lables\n",
    "# #np.bincount() is to count unique value of y \n",
    "# print(np.bincount(y_train))\n",
    "\n",
    "#5. standardize features from train and test datasets to mean 0 and variance 1 (train the scaler only on train data)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_scaled = sc.transform(X_train)\n",
    "#print('means:', X_train.mean(axis=0), X_train_scaled.mean(axis=0))\n",
    "#print('sigmas', X_train.std(axis=0), X_train_scaled.std(axis=0))\n",
    "\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "#print('means:', X_test.mean(axis=0), X_test_scaled.mean(axis=0))\n",
    "#print('sigmas', X_test.std(axis=0), X_test_scaled.std(axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2 - (30 Points) {-}\n",
    "\n",
    "6. Using approapriate `scikit-learn` libararies we learned in class to fit the following classifiers to the training dataset constructed in Problem 1. \n",
    "    - Logistic Regression - name your instance `lr` set `random_state=1`\n",
    "    - Support Vector Machine with Linear Kernel - name your instance `svm_linear` set `C=1.0` and `random_state=1`\n",
    "    - Support Vector Machine with RBF Kernel - name your instance `svm_rbf` set `gamma = 10`, `C=10.0`, `random_state=1`\n",
    "    - Decision Tree - name your instance `tree` set `criterion='gini'`, `max_depth = 4`, `random_state=1`\n",
    "    - Random Forest - name your instance  `forest` set `criterion='gini'`, `n_estimators=50`, `random_state=1`\n",
    "    - KNN - name your instance `knn` set `n_neighbors=5`, `p=2`, `metric='minkowski'`\n",
    "    \n",
    "When initializing instances of the above classifiers only set parameters provided above and leave all other parameters equal to their `scikit-learn` default values.  \n",
    "(30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your code for Problem 2 in the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Training the classifier by Logistic Regression \n",
    "from sklearn .linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=1)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "#2.Training the classifier by Support Vector Machine - Linear Kernel \n",
    "from sklearn.svm import SVC\n",
    "svm_linear = SVC(kernel='linear', C=1.0, random_state=1)\n",
    "svm_linear.fit(X_train_scaled, y_train)\n",
    "\n",
    "#3.Training the classifier by Support Vector Machine - RBF Kernel \n",
    "svm_rbf = SVC(kernel='rbf', gamma=10, C=10.0, random_state=1)\n",
    "svm_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "#4.Training the classifier by Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "#5.Training the classifier by Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier (criterion = 'gini', n_estimators=50, random_state=1)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "#6.Training the classifier by KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3 - (40 points) {-}\n",
    "\n",
    "7. Using a method built into each of the above classifiers, compute prediction accuracy on train data for each classifier and store it into variables named according to the following pattern: `classifier_name_accuracy_train`, for instance you should have `lr_accuracy_train` and `forest_accuracy_train`. (10 points)\n",
    "\n",
    "8. Using a method built into each of the above classifiers, compute prediction accuracy on test data for each classifier and store it into variables named according to the following pattern: `classifier_name_accuracy_test`, for instance you should have `lr_accuracy_test` and `forest_accuracy_test`. (10 points)\n",
    "\n",
    "9. Which methods rank in the first two places according to their ability to accurately classify train data, and which two methods perform worst on train dataset? (Make sure you format your text nicely using markup language in the cell provided below) (10 points)\n",
    "\n",
    "10. Which methods rank in the first two places according to their ability to accurately classify test data, and which two methods perform worst on test dataset? Are these the same models we found in Question 9? Is this expected? (Make sure you format your text nicely using markup language in the cell provided below) (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your code for Problem 2 (Questions 7 and 8) in the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Compute prediction accuracy on train data \n",
    "##1. Logistic Regression\n",
    "lr_accuracy_train = lr.score(X_train_scaled, y_train)\n",
    "\n",
    "##2. SVM - Linear\n",
    "svm_linear_accuracy_train = svm_linear.score(X_train_scaled, y_train)\n",
    "\n",
    "##3. SVM - RBF\n",
    "svm_rbf_accuracy_train = svm_rbf.score(X_train_scaled, y_train)\n",
    "\n",
    "##4. Decision Tree\n",
    "tree_accurary_train = tree.score(X_train, y_train)\n",
    "\n",
    "##5. Random Forest\n",
    "forest_accuracy_train = forest.score(X_train, y_train)\n",
    "\n",
    "##6. KNN\n",
    "knn_accuracy_train = knn.score(X_train_scaled, y_train)\n",
    "\n",
    "#8. Compute prediction accurary on test data\n",
    "##1. Logistic Regression\n",
    "lr_accuracy_test = lr.score(X_test_scaled, y_test)\n",
    "\n",
    "##2. SVM - Linear\n",
    "svm_linear_accuracy_test = svm_linear.score(X_test_scaled, y_test)\n",
    "\n",
    "##3. SVM - RBF\n",
    "svm_rbf_accuracy_test = svm_rbf.score(X_test_scaled, y_test)\n",
    "\n",
    "##4. Decision Tree\n",
    "tree_accurary_test = tree.score(X_test, y_test)\n",
    "\n",
    "##5. Random Forest\n",
    "forest_accuracy_test = forest.score(X_test, y_test)\n",
    "\n",
    "##6. KNN\n",
    "knn_accuracy_test = knn.score(X_test_scaled, y_test)\n",
    "\n",
    "# #----Testing: \n",
    "# print(f'Accuracy for train LR = {lr_accuracy_train:.3f}')\n",
    "# print(f'Accuracy for train SVM linear = {svm_linear_accuracy_train:.3f}')\n",
    "# print(f'Accuracy for train SVM RBF = {svm_rbf_accuracy_train:.3f}')\n",
    "# print(f'Accuracy for train Decicion Tree = {tree_accurary_train:.3f}')\n",
    "# print(f'Accuracy for train Random Forest = {forest_accuracy_train:.3f}')\n",
    "# print(f'Accuracy for train KNN = {knn_accuracy_train:.3f}')\n",
    "# print('-------------------------')\n",
    "# print(f'Accuracy for test LR = {lr_accuracy_test:.3f}')\n",
    "# print(f'Accuracy for test SVM linear = {svm_linear_accuracy_test:.3f}')\n",
    "# print(f'Accuracy for test SVM RBF = {svm_rbf_accuracy_test:.3f}')\n",
    "# print(f'Accuracy for test Decision Tree = {tree_accurary_test:.3f}')\n",
    "# print(f'Accuracy for test Random Forest = {forest_accuracy_test:.3f}')\n",
    "# print(f'Accuracy for test KNN = {knn_accuracy_test:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answers for Problem 2 (Questions 9 and 10) in the cell below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 9:**\n",
    "\n",
    "According to the ability to accurately classify training dataset: \n",
    "- The methods rank in the first two places are **Random Forest** (accurary = 1) and **SVM with RBF Kernel** (accuracy = 0.989). \n",
    "\n",
    "- The two methods perform worst on training dataset are **Logistic Regression** and **SVM with Linear Kernel** (accuracy = 0.771). \n",
    "\n",
    "**Answer 10:**\n",
    "\n",
    "In term of test dataset:\n",
    "- The methods rank in the first two places are **SVM with Linear Kernel** (accuracy = 0.773) and **Logistic Regression** (accurary = 0.747). \n",
    "\n",
    "- The two methods perform worst on train dataset are **SVM with RBF Kernel** (accuracy = 0.707) and **Random Forest** (accuracy = 0.720). \n",
    "\n",
    "These are the same models we found in Question 9, but in the reverse order where the two methods in the first rank of the train dataset performed worst in the test dataset and vice versa. This is **not** expected. Basically we use test dataset to confirm the validity of a machine learning algorithm and expect that the model performing well on trainning data should perform well on the test data. However, there is a sign of overfitting in the models Random Forest and SVM with RBF Kernel. Indeed, these 2 methods fit the training dataset too closely (very high accuracy rate). They are generally too complicated and pick up any random variables in training dataset. Hence, for unseen dataset like test dataset, they did not generalize well. \n",
    "\n",
    "Meanwhile, although the Logistic Regression and SVM with Linear Kernel models have lower accuracy rate in train dataset than other models, their accuracy on test dataset are almost as same as the one in training dataset, which means that they perform well in capture unseen data in comparison to other methods. \n",
    "\n",
    "Therefore, it is importance to split the dataset into trainning and test datasets, so that we can use test dataset to confirm the effectiveness of the selected model on unseen data and avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
